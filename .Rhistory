#load the training dataset
subject_train <- read.table(paste(dir, "/train/subject_train.txt", sep = ""))
X_train <- read.table(paste(dir, "/train/X_train.txt", sep = ""))
y_train <- read.table(paste(dir, "/train/y_train.txt", sep = ""))
#'
#' 1-Merges the training and the test sets to create one data set.
#'
subject_all <- rbind(subject_test, subject_train)
#load the activity names
activity_labels <- read.table(paste(dir, "/activity_labels.txt", sep = ""))$V2
#' 4-Appropriately labels the data set with descriptive variable names.
names(X_test) <- features
names(X_train) <- features
#' 2-Extracts only the measurements on the mean and standard deviation for each measurement.
mean_and_std <- grepl("mean\\(\\)|std\\(\\)", features)
X_test_mean_and_std <- X_test[,mean_and_std]
X_train_mean_and_std <- X_train[,mean_and_std]
#merge all test and train rows
X_all <- rbind(X_test_mean_and_std, X_train_mean_and_std)
y_all <- rbind(y_test, y_train)
#combine all vectors/data.frames into one data.frame
merged <- cbind(subject_all, y_all, X_all)
#' Uses descriptive activity names to name the activities in the data set
names(merged)[1] <- "SubjectID"
names(merged)[2] <- "Activity"
#' Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
tidy_data <- aggregate(. ~ SubjectID + Activity, data=merged, FUN = mean)
#give activities better names
tidy_data$Activity <- factor(tidy_data$Activity, labels=activity_labels)
write.table(tidy_data, file="./tidy_data.txt", sep="\t", row.names=FALSE)
#'
#' You should create one R script called run_analysis.R that does the
#' following.
#'
#' 1-Merges the training and the test sets to create one data set.
#' 2-Extracts only the measurements on the mean and standard deviation for each measurement.
#' 3-Uses descriptive activity names to name the activities in the data set
#' 4-Appropriately labels the data set with descriptive variable names.
#' 5-Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
#'
#'
#' Here is the data set for the project:
#'
#' https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
#' make sure everything is set up correctly and is clean
if (!require("reshape2")) {
install.packages("reshape2")
}
library(reshape2)
# Clean up workspace
rm(list=ls())
####### helper function to download and unzip data file if needed
downloadAndUnzipFile <- function(zipUrl, destDir) {
destDir
if (!file.exists(destDir)) {
temp <- tempfile()
download.file(url = zipUrl, destfile = temp)
unzip(temp)
unlink(temp)
}
}
#' Download the the data file and unzip it if needed
#'
########################################################################
#'                            PLEASE NOTE
#' If in the work directory there is a directory named "UCI HAR Dataset"
#' it is assumed that the dataset was previously downloaded and extracted
#' there. If it is not found, data will be downloaded unziped.
#'
#########################################################################
url <- "http://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
dir <- "./UCI HAR Dataset"
downloadAndUnzipFile(url, dir)
#load the test dataset
subject_test <- read.table(paste(dir, "/test/subject_test.txt", sep = ""))
X_test <- read.table(paste(dir, "/test/X_test.txt", sep=""))
y_test <- read.table(paste(dir, "/test/y_test.txt", sep =""))
#load the feature names
features <- read.table(paste(dir, "/features.txt", sep = ""))$V2
#load the training dataset
subject_train <- read.table(paste(dir, "/train/subject_train.txt", sep = ""))
X_train <- read.table(paste(dir, "/train/X_train.txt", sep = ""))
y_train <- read.table(paste(dir, "/train/y_train.txt", sep = ""))
#'
#' 1-Merges the training and the test sets to create one data set.
#'
subject_all <- rbind(subject_test, subject_train)
#load the activity names
activity_labels <- read.table(paste(dir, "/activity_labels.txt", sep = ""))$V2
#' 4-Appropriately labels the data set with descriptive variable names.
names(X_test) <- features
names(X_train) <- features
#' 2-Extracts only the measurements on the mean and standard deviation for each measurement.
mean_and_std <- grepl("mean\\(\\)|std\\(\\)", features)
X_test_mean_and_std <- X_test[,mean_and_std]
X_train_mean_and_std <- X_train[,mean_and_std]
#merge all test and train rows
X_all <- rbind(X_test_mean_and_std, X_train_mean_and_std)
y_all <- rbind(y_test, y_train)
#combine all vectors/data.frames into one data.frame
merged <- cbind(subject_all, y_all, X_all)
#' Uses descriptive activity names to name the activities in the data set
names(merged) <- c("SubjectID", "Activity")
#' Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
tidy_data <- aggregate(. ~ SubjectID + Activity, data=merged, FUN = mean)
#give activities better names
tidy_data$Activity <- factor(tidy_data$Activity, labels=activity_labels)
write.table(tidy_data, file="./tidy_data.txt", sep="\t", row.names=FALSE)
#'
#' You should create one R script called run_analysis.R that does the
#' following.
#'
#' 1-Merges the training and the test sets to create one data set.
#' 2-Extracts only the measurements on the mean and standard deviation for each measurement.
#' 3-Uses descriptive activity names to name the activities in the data set
#' 4-Appropriately labels the data set with descriptive variable names.
#' 5-Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
#'
#'
#' Here is the data set for the project:
#'
#' https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
#' make sure everything is set up correctly and is clean
if (!require("reshape2")) {
install.packages("reshape2")
}
library(reshape2)
# Clean up workspace
rm(list=ls())
####### helper function to download and unzip data file if needed
downloadAndUnzipFile <- function(zipUrl, destDir) {
destDir
if (!file.exists(destDir)) {
temp <- tempfile()
download.file(url = zipUrl, destfile = temp)
unzip(temp)
unlink(temp)
}
}
#' Download the the data file and unzip it if needed
#'
########################################################################
#'                            PLEASE NOTE
#' If in the work directory there is a directory named "UCI HAR Dataset"
#' it is assumed that the dataset was previously downloaded and extracted
#' there. If it is not found, data will be downloaded unziped.
#'
#########################################################################
url <- "http://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
dir <- "./UCI HAR Dataset"
downloadAndUnzipFile(url, dir)
#load the test dataset
subject_test <- read.table(paste(dir, "/test/subject_test.txt", sep = ""))
X_test <- read.table(paste(dir, "/test/X_test.txt", sep=""))
y_test <- read.table(paste(dir, "/test/y_test.txt", sep =""))
#load the feature names
features <- read.table(paste(dir, "/features.txt", sep = ""))$V2
#load the training dataset
subject_train <- read.table(paste(dir, "/train/subject_train.txt", sep = ""))
X_train <- read.table(paste(dir, "/train/X_train.txt", sep = ""))
y_train <- read.table(paste(dir, "/train/y_train.txt", sep = ""))
#'
#' 1-Merges the training and the test sets to create one data set.
#'
subject_all <- rbind(subject_test, subject_train)
#load the activity names
activity_labels <- read.table(paste(dir, "/activity_labels.txt", sep = ""))$V2
#' 4-Appropriately labels the data set with descriptive variable names.
names(X_test) <- features
names(X_train) <- features
#' 2-Extracts only the measurements on the mean and standard deviation for each measurement.
mean_and_std <- grepl("mean\\(\\)|std\\(\\)", features)
X_test_mean_and_std <- X_test[,mean_and_std]
X_train_mean_and_std <- X_train[,mean_and_std]
#merge all test and train rows
X_all <- rbind(X_test_mean_and_std, X_train_mean_and_std)
y_all <- rbind(y_test, y_train)
#combine all vectors/data.frames into one data.frame
merged <- cbind(subject_all, y_all, X_all)
#' Uses descriptive activity names to name the activities in the data set
names(merged)[1:2] <- c("SubjectID", "Activity")
#' Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
tidy_data <- aggregate(. ~ SubjectID + Activity, data=merged, FUN = mean)
#give activities better names
tidy_data$Activity <- factor(tidy_data$Activity, labels=activity_labels)
write.table(tidy_data, file="./tidy_data.txt", sep="\t", row.names=FALSE)
#'
#' You should create one R script called run_analysis.R that does the
#' following.
#'
#' 1-Merges the training and the test sets to create one data set.
#' 2-Extracts only the measurements on the mean and standard deviation for each measurement.
#' 3-Uses descriptive activity names to name the activities in the data set
#' 4-Appropriately labels the data set with descriptive variable names.
#' 5-Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
#'
#'
#' Here is the data set for the project:
#'
#' https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
#' make sure everything is set up correctly and is clean
if (!require("reshape2")) {
install.packages("reshape2")
}
library(reshape2)
# Clean up workspace
rm(list=ls())
####### helper function to download and unzip data file if needed
downloadAndUnzipFile <- function(zipUrl, destDir) {
destDir
if (!file.exists(destDir)) {
temp <- tempfile()
download.file(url = zipUrl, destfile = temp)
unzip(temp)
unlink(temp)
}
}
#' Download the the data file and unzip it if needed
#'
########################################################################
#'                            PLEASE NOTE
#' If in the work directory there is a directory named "UCI HAR Dataset"
#' it is assumed that the dataset was previously downloaded and extracted
#' there. If it is not found, data will be downloaded unziped.
#'
#########################################################################
url <- "http://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
dir <- "./UCI HAR Dataset"
downloadAndUnzipFile(url, dir)
#load the test dataset
subject_test <- read.table(paste(dir, "/test/subject_test.txt", sep = ""))
X_test <- read.table(paste(dir, "/test/X_test.txt", sep=""))
y_test <- read.table(paste(dir, "/test/y_test.txt", sep =""))
#load the feature names
features <- read.table(paste(dir, "/features.txt", sep = ""))$V2
#load the training dataset
subject_train <- read.table(paste(dir, "/train/subject_train.txt", sep = ""))
X_train <- read.table(paste(dir, "/train/X_train.txt", sep = ""))
y_train <- read.table(paste(dir, "/train/y_train.txt", sep = ""))
#'
#' 1-Merges the training and the test sets to create one data set.
#'
subject_all <- rbind(subject_test, subject_train)
#load the activity names
activity_labels <- read.table(paste(dir, "/activity_labels.txt", sep = ""))$V2
#' 4-Appropriately labels the data set with descriptive variable names.
names(X_test) <- features
names(X_train) <- features
#' 2-Extracts only the measurements on the mean and standard deviation for each measurement.
mean_and_std <- grepl("mean\\(\\)|std\\(\\)", features)
X_test_mean_and_std <- X_test[,mean_and_std]
X_train_mean_and_std <- X_train[,mean_and_std]
#merge all test and train rows
X_all <- rbind(X_test_mean_and_std, X_train_mean_and_std)
y_all <- rbind(y_test, y_train)
#combine all vectors/data.frames into one data.frame
merged <- cbind(subject_all, y_all, X_all)
#' Uses descriptive activity names to name the activities in the data set
names(merged)[1:2] <- c("SubjectID", "Activity")
#' Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
tidy_data <- aggregate(. ~ SubjectID + Activity, data=merged, FUN = mean)
#give activities better names
tidy_data$Activity <- factor(tidy_data$Activity, labels=activity_labels)
write.table(tidy_data, file="./tidy_data.txt", sep="\t", row.names=FALSE)
#### That is all folks!
#'
#' You should create one R script called run_analysis.R that does the
#' following.
#'
#' 1-Merges the training and the test sets to create one data set.
#' 2-Extracts only the measurements on the mean and standard deviation for each measurement.
#' 3-Uses descriptive activity names to name the activities in the data set
#' 4-Appropriately labels the data set with descriptive variable names.
#' 5-Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
#'
#'
#' Here is the data set for the project:
#'
#' https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
#' make sure everything is set up correctly and is clean
if (!require("reshape2")) {
install.packages("reshape2")
}
library(reshape2)
# Clean up workspace
rm(list=ls())
####### helper function to download and unzip data file if needed
downloadAndUnzipFile <- function(zipUrl, destDir) {
destDir
if (!file.exists(destDir)) {
temp <- tempfile()
download.file(url = zipUrl, destfile = temp)
unzip(temp)
unlink(temp)
}
}
#' Download the the data file and unzip it if needed
#'
########################################################################
#'                            PLEASE NOTE
#' If in the work directory there is a directory named "UCI HAR Dataset"
#' it is assumed that the dataset was previously downloaded and extracted
#' there. If it is not found, data will be downloaded unziped.
#'
#########################################################################
url <- "http://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
dir <- "./UCI HAR Dataset"
downloadAndUnzipFile(url, dir)
#load the test dataset
subject_test <- read.table(paste(dir, "/test/subject_test.txt", sep = ""))
X_test <- read.table(paste(dir, "/test/X_test.txt", sep=""))
y_test <- read.table(paste(dir, "/test/y_test.txt", sep =""))
#load the feature names
features <- read.table(paste(dir, "/features.txt", sep = ""))$V2
#load the training dataset
subject_train <- read.table(paste(dir, "/train/subject_train.txt", sep = ""))
X_train <- read.table(paste(dir, "/train/X_train.txt", sep = ""))
y_train <- read.table(paste(dir, "/train/y_train.txt", sep = ""))
#'
#' 1-Merges the training and the test sets to create one data set.
#'
subject_all <- rbind(subject_test, subject_train)
#load the activity names
activity_labels <- read.table(paste(dir, "/activity_labels.txt", sep = ""))$V2
#' 4-Appropriately labels the data set with descriptive variable names.
names(X_test) <- features
names(X_train) <- features
#' 2-Extracts only the measurements on the mean and standard deviation for each measurement.
mean_and_std <- grepl("mean\\(\\)|std\\(\\)", features)
X_test_mean_and_std <- X_test[,mean_and_std]
X_train_mean_and_std <- X_train[,mean_and_std]
#merge all test and train rows
X_all <- rbind(X_test_mean_and_std, X_train_mean_and_std)
y_all <- rbind(y_test, y_train)
#combine all vectors/data.frames into one data.frame
merged <- cbind(subject_all, y_all, X_all)
#' Uses descriptive activity names to name the activities in the data set
names(merged)[1:2] <- c("SubjectID", "Activity")
#' Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
tidy_data <- aggregate(. ~ SubjectID + Activity, data=merged, FUN = mean)
#give activities better names
tidy_data$Activity <- factor(tidy_data$Activity, labels=activity_labels)
write.table(tidy_data, file="./tidy_data.txt", sep="\t", row.names=FALSE)
#### That is all folks!
library(sweave)
install.packages("sweave")
install.packages("knitr")
library(knitr)
names(knitr)
str(knitr)
functions(knitr)
GOOG
AAPL
download_price(GLD)
download_price <- function(stock_name, start_date=NULL, end_date=NULL) {
stock <- read.csv(
paste("http://real-chart.finance.yahoo.com/table.csv?s=",
stock_name, "&a=00&b=1&c=2005&d=06&e=20&f=2014&g=d&ignore=.csv",
sep =""),
header=T, sep = ",")
assign(stock_name[1], stock, envir = .GlobalEnv)
}
download_price(GLD)
download_price("GLD")
GLD
head(GLD)
library(kernlab)
install.packages("kernlab")
library(kernlab)
data(spam)
names(spam)
head(spam)
trainIndicator = rbinom(4601, size = 1, prob = 0.5)
str(trainIndicator)
trainIndicator
table(trainIndicator)
set.seed(3435)
trainIndicator = rbinom(4601, size = 1, prob = 0.5)
table(trainIndicator)
trainSpam = spam[trainIndicator == 1,]
testSpam = spam[trainIndicator == 0,]
names(trainSpam)
trainSpam$type
trainSpam$type == 'spam'
trainSpam$type == spam
trainSpam$type
trainSpam$type == as.data.frame.factor('spam')
trainSpam$type == as.data.frame.factor(spam)
as.factor("spam")
trainSpam$type == as.factor('spam')
class(trainSpam$type)
class(as.factor("spam"))
trainSpam$type
as.factor("spam")
trainSpam$type[,1]
trainSpam$type[1]
trainSpam[,]
trainSpam$type == as.factor("spam")
levels(trainSpam$type)
as.factor(trainSpam$type)
as.factor(trainSpam$type) == as.factor("spam")
as.factor("spam") %in% trainSpam$type
as.factor("spam") %in% trainSpam$type[1,1]
trainSpam[,type]
trainSpam[,type == as.factor("spam")]
trainSpam[trainSpam$type == as.factor("spam"),]
trainSpam$type %in% as.factor("spam")
which(trainSpam$type %in% as.factor("spam"))
trainSpam[which(trainSpam$type %in% as.factor("spam")),]
trainSpam[which(trainSpam$type %in% as.factor("nospam")),]
head(trainSpam)
levels(trainSpam$type)
trainSpam[which(trainSpam$type %in% as.factor("nonspam")),]
names(trainSpam)
names(trainSpam)[1:57]
trainSpam[1, "our"]
trainSpam[, "our"]
trainSpam[, c("our", "type")]
trainSpam[,names(trainSpam)[1:57]]
as.numeric(trainSpam[,names(trainSpam)[1:57]])
mean(trainSpam[,names(trainSpam)[1:57]])
trainSpam$numType = as.numeric(trainSpam$type) - 1
trainSpam$numType
costFunction = function(x, y) sum(x != (y>0.5))
x <- 1:4
p <- x/sum(x)
temp <- rbin(x,p)
temp <- rbind(x,p)
rownames(temp) <- c("X", "Prob")
temp
max_interval <- names(which(z == max(z)))[1]
getwd()
history
hist()
history()
setwd("/Users/kaamel/Google\ Drive/Classes/coursera/Reproducible\ Research/RepData_PeerAssessment1")
max_interval <- names(which(z == max(z)))[1]
unzip("activity.zip", files = "activity.csv", overwrite = T)
data <- read.csv("activity.csv")
z <- tapply(na.omit(data)$steps, FUN = function(x) {mean(x)}, INDEX = as.factor(na.omit(data)$interval))
max_interval <- names(which(z == max(z)))[1]
max_interval
as.character(max_interval)
class(max_interval)
levels(factors(z$interval))
factors(z$interval)
z$interval
z
levels(factors(na.omit(data)$interval))
na.omit(data)$interval
factor(na.omit(data)$interval)
levels(factor(na.omit(data)$interval))
z$835
z$"835"
z[104]
z[(which(z == max(z)))[1]]
z[(which(z == max(z)))[1]][1]
na.fail(data)
na.fail(data$steps)
na.omit(data)
length(na.omit(data))
ncol(data)
data
class(data)
class(data$interval)
class(data$steps)
class(data$date)
read.table(file="activity.csv")
read.table(file="activity.csv", header = T)
table(data)
names(table(data)
)
head(table(data))
table(data, interval)
as.table(data)
activity_data <- read.csv("activity.csv")
table(activity_data)
is.na(data)
colSum(is.na(data_activity))
length(is.na(data_activity))
length(is.na(activity_data))
length(activity_data)
lis.na(data)
length(is.na(data_activity))
length(is.na(activity_data))
length(!is.na(activity_data))
nrow(data)
nrow(na.omit(data))
nrow(data) - nrow(na.omit(data))
data %in% nrow(na.omit(data)
)
na.fail(activity_data)
na.pass(activity_data)
is.na(activity_data)
nrow(is.na(activity_data)
)
nrow(na.omit(activity_data)
)
nrow(!na.omit(activity_data))
nrow(!is.na(activity_data)
)
nrow(is.na(activity_data))
data[,1]
data[which(is.na(data)),]
head(data[which(is.na(data)),])
z[0]
z[1]
head(z)
data[which(is.na(data)),]$interval
news_data <- data
new_data <- data
is.na(new_data)
